{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of training onet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_90SOwP3jteE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaK6xl1ujmIT"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "# Torch\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "# TB\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# CV2\n",
        "import cv2\n",
        "# TQDM\n",
        "from tqdm import tqdm\n",
        "# Summary\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_90SOwP3jteE"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1U9ngjejwi0"
      },
      "source": [
        "# Dataset\n",
        "class CTDataset(Dataset):\n",
        "  def __init__(self, images, masks, transform=None):\n",
        "      self.images = images\n",
        "      self.masks = masks\n",
        "      self.transform = transform\n",
        "      #print(len(self.data))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image = self.images[index]\n",
        "    mask = self.masks[index]\n",
        "\n",
        "    if self.transform is not None:\n",
        "      image = image.astype(np.float32)\n",
        "      mask = mask.astype(np.float32)\n",
        "      #print(image.shape, image.dtype, mask.shape, mask.dtype)\n",
        "      augmentation = self.transform(image=image, mask=mask)\n",
        "      image = augmentation['image']\n",
        "      mask = augmentation['mask']\n",
        "      return image.float(), mask.float().unsqueeze(0)\n",
        "\n",
        "    return np.expand_dims(image.astype(np.float32), axis=0), np.expand_dims(mask.astype(np.float32), axis=0)\n",
        "\n",
        "def test_dataset():\n",
        "  # No augmentation\n",
        "  no_aug = CTDataset(val_images[:10], val_masks[:10])\n",
        "  image, mask = no_aug[0]\n",
        "  print(image.shape, image.dtype, mask.shape, mask.dtype)\n",
        "\n",
        "  # With Augmentation\n",
        "  testing_transformer = A.Compose([\n",
        "            #A.Resize(height=160, width=160),\n",
        "            #A.RandomCrop(width=500, height=500),\n",
        "            A.Rotate(limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=-2000),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            #A.VerticalFlip(p=0.1),\n",
        "            #A.Blur(blur_limit=1, p=0.5),\n",
        "            # A.Normalize(\n",
        "            #     mean=[0.0, 0.0, 0.0],\n",
        "            #     std=[1.0, 1.0, 1.0],\n",
        "            #     max_pixel_value=255.0,\n",
        "            # ),\n",
        "            ToTensorV2(),\n",
        "            ])\n",
        "  aug = CTDataset(val_images[:10], val_masks[:10], testing_transformer)\n",
        "  image, mask = aug[5]\n",
        "  print(image.shape, image.dtype, mask.shape, mask.dtype)\n",
        "  plt.imshow(image[0], cmap=\"gray\", origin=\"lower\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  pass\n",
        "  #test_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMS6X_isjqIv"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR5_eQmxx2il"
      },
      "source": [
        "# Onet model\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        return F.relu(self.bn2(self.conv2(out)))\n",
        "\n",
        "\n",
        "class ONET(nn.Module):\n",
        "    def mp(self, x, kernel=2, stride=2):\n",
        "        return F.max_pool2d(x, kernel, stride)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=1,\n",
        "        out_channels=1,\n",
        "        features=[64, 32]\n",
        "        ):\n",
        "        super(ONET, self).__init__()\n",
        "        self.top_en = nn.ModuleList()\n",
        "        self.top_de = nn.ModuleList()\n",
        "        self.btm_en = nn.ModuleList()\n",
        "        self.btm_de = nn.ModuleList()\n",
        "\n",
        "        # self.ups = nn.ModuleList()\n",
        "        # self.downs = nn.ModuleList()\n",
        "        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Right part of ONET\n",
        "        for feature in features:\n",
        "            self.top_en.append(DoubleConv(in_channels, feature))\n",
        "            self.top_en.append(nn.ConvTranspose2d(feature, feature, kernel_size=2, stride=2))\n",
        "\n",
        "            self.btm_en.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Left part of ONET\n",
        "        for feature in reversed(features):\n",
        "            self.top_de.append(nn.Conv2d(feature//2, feature, 3, 1, 1))\n",
        "            self.top_de.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "            self.btm_de.append(nn.ConvTranspose2d(feature//2, feature, kernel_size=2, stride=2))\n",
        "            self.btm_de.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        # Bottleneck\n",
        "        self.top_bottleneck = DoubleConv(features[-1], features[-1]//2)\n",
        "        self.btm_bottleneck = DoubleConv(features[-1], features[-1]//2)\n",
        "\n",
        "        #Output\n",
        "        self.final_conv = nn.Conv2d(features[0]*2, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, t = x, x\n",
        "        top_skip_connections = []\n",
        "        btm_skip_connections = []\n",
        "\n",
        "        # Bottom part of the ONET\n",
        "\n",
        "        for down in self.btm_en:\n",
        "            b = down(b)\n",
        "            btm_skip_connections.append(b)\n",
        "            b = self.mp(b)\n",
        "\n",
        "        b = self.btm_bottleneck(b)\n",
        "        btm_skip_connections = btm_skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.btm_de), 2):\n",
        "            b = self.btm_de[idx](b)\n",
        "            skip_connection = btm_skip_connections[idx//2]\n",
        "            if b.shape != skip_connection.shape:\n",
        "                b = TF.resize(b, size=skip_connection.shape[2:])\n",
        "            concat_skip = torch.cat((skip_connection, b), dim=1)\n",
        "            b = self.btm_de[idx+1](concat_skip)\n",
        "        # Top part of the ONET\n",
        "        save = False\n",
        "        for idx, up in enumerate(self.top_en):\n",
        "            t = up(t)\n",
        "            # print(idx, t.shape)\n",
        "            if idx%2 !=0:\n",
        "              # print('saved:', idx)\n",
        "              top_skip_connections.append(t)\n",
        "            save = !save\n",
        "        t = self.top_bottleneck(t)\n",
        "        # decoder\n",
        "\n",
        "        top_skip_connections = top_skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.top_de), 2):\n",
        "            t = self.top_de[idx](t)\n",
        "            skip_connection = top_skip_connections[idx//2]\n",
        "            # print('t', idx, t.shape)\n",
        "            # print('sc:', idx//2, skip_connection.shape)\n",
        "\n",
        "            if t.shape != skip_connection.shape:\n",
        "                # print('resizing')\n",
        "                t = TF.resize(t, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, t), dim=1)\n",
        "            # print('concat: ', concat_skip.shape)\n",
        "            t = self.mp(self.top_de[idx+1](concat_skip))\n",
        "\n",
        "            # print('ft:', idx, t.shape)\n",
        "        # print(t.shape, b.shape, 'suuup')\n",
        "        x = torch.cat((t, b), dim=1)\n",
        "        return self.final_conv(x)\n",
        "\n",
        "def test_model_o():\n",
        "    x = torch.randn((5, 1, 160, 160)).to('cpu')\n",
        "    model = ONET(1,1, features=[32,16]).to('cpu')\n",
        "    preds = model(x)\n",
        "    print('input ', preds.shape)\n",
        "    print('output: ', x.shape)\n",
        "    summary(model, input_size=(1, 160, 160))\n",
        "    assert preds.shape == x.shape\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n",
        "    #test_model_o()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLJjc8o87Bc3"
      },
      "source": [
        "# Load and save the model\n",
        "def save_checkpoint(state, filename=\"CTUnet.pth.tar\"):\n",
        "    print(f\"[*] Saving checkpoint: {filename}\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(f\"[*] Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLYmr0GBMjkK"
      },
      "source": [
        "# Building data loaders\n",
        "def get_loaders(\n",
        "    train_images,\n",
        "    train_masks,\n",
        "    val_images,\n",
        "    val_masks,\n",
        "    train_batch_size, \n",
        "    val_batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True):\n",
        "  \n",
        "  # Training data loader\n",
        "  train_ds = CTDataset(\n",
        "        images=train_images,\n",
        "        masks=train_masks, \n",
        "        transform=train_transform)\n",
        "  train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=train_batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True)\n",
        "  \n",
        "  # Validation data loader\n",
        "  val_ds = CTDataset(\n",
        "        images=val_images,\n",
        "        masks=val_masks, \n",
        "        transform=val_transform)\n",
        "  val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=val_batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False)\n",
        "    \n",
        "  return train_loader, val_loader\n",
        "\n",
        "def test_loader():\n",
        "  transformer = A.Compose([\n",
        "            #A.Resize(height=512, width=512),\n",
        "            #A.RandomCrop(width=500, height=500),\n",
        "            #A.Rotate(limit=15, p=.9),\n",
        "            #A.VerticalFlip(p=0.1),\n",
        "            A.Blur(blur_limit=1, p=0.5),\n",
        "            ToTensorV2()])\n",
        "  train_loader, val_loader = get_loaders(train_images[:10], train_masks[:10], val_images[:10], val_masks[:10], 1, 5, transformer, transformer, 2)\n",
        "  print(train_loader, val_loader)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  pass\n",
        "  #test_loader()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPjXZAZCNB7f"
      },
      "source": [
        "# Checking accuracy \n",
        "def check_accuracy(model, loader, device=\"cuda\", SMOOTH = 1e-8):\n",
        "  torch.cuda.empty_cache()\n",
        "  print('[*] Evaluating the model')\n",
        "\n",
        "  # Met\n",
        "  pixAcc_score = 0\n",
        "  dice_score = 0\n",
        "  iou_score = 0\n",
        "  jaccard_score = 0\n",
        "\n",
        "  pres_score = 0\n",
        "  sens_score = 0\n",
        "  spec_score = 0\n",
        "  g_mean_score = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      if torch.cuda.is_available():\n",
        "        x = x.cuda().float()\n",
        "      else:\n",
        "        x = x.float()\n",
        "\n",
        "      y = y.squeeze(1).int().to('cuda')\n",
        "      preds = model(x)\n",
        "      x.detach_()\n",
        "\n",
        "      preds = torch.sigmoid(preds).squeeze(1)\n",
        "      preds = (preds >= 0.5).int()\n",
        "\n",
        "      cn = confusion_matrix(torch.flatten(y).cpu(), torch.flatten(preds).cpu())\n",
        "      # Precision\n",
        "      pres = cn[0][0] / (cn[0][0]+cn[0][1])\n",
        "      # Sensitivity\n",
        "      sens = cn[0][0] / (cn[0][0]+cn[1][1])\n",
        "      # Specificity\n",
        "      spec = cn[1][0] / (cn[1][0] + cn[0][1])\n",
        "      # G-means\n",
        "      g_mean_score += math.sqrt(sens * spec)\n",
        "      # Adding the scores\n",
        "      pres_score += pres\n",
        "      sens_score += sens\n",
        "      spec_score += spec\n",
        "\n",
        "      # Pixel Accuracy\n",
        "      num_correct = (preds == y).sum()\n",
        "      num_pixels = torch.numel(preds)\n",
        "\n",
        "      pixAcc_score += num_correct/num_pixels\n",
        "\n",
        "      # IoU: The area of overlap between the predicted segmentation and the ground truth divided by the area of union between the predicted segmentation and the ground truth\n",
        "      intersection = (preds & y).float().sum()\n",
        "      union = (preds | y).float().sum()\n",
        "      iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
        "      iou_score += iou\n",
        "\n",
        "      # # Jaccard Index\n",
        "      sum = preds.sum() + y.sum() - intersection\n",
        "      jaccard_score += intersection / (sum + SMOOTH)\n",
        "\n",
        "      # Dice Coefficient (F1 Score): 2 * The area of Overlap divided by the total number of pixels in both images\n",
        "      dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + SMOOTH)\n",
        "      plt.imshow(preds[0].cpu(), cmap=\"gray\", origin=\"lower\")\n",
        "      preds.detach_()\n",
        "      y.detach_()\n",
        "      del y, preds\n",
        "      torch.cuda.empty_cache()\n",
        "    print(f'[!] Pixel Accuracy: {pixAcc_score / len(loader)}')\n",
        "    print(f'[!] Dice Coef, F1: {dice_score/len(loader)}')\n",
        "    print(f'[!] IoU: {iou_score / len(loader)}')\n",
        "    print(f'[!] Jaccard: {jaccard_score / len(loader)}')\n",
        "    print(f'[!] Precision: {pres_score / len(loader)}')\n",
        "    print(f'[!] Sensitivity: {sens_score / len(loader)}')\n",
        "    print(f'[!] Specificity: {spec_score / len(loader)}')\n",
        "    print(f'[!] G-means: {g_mean_score / len(loader)}')\n",
        "    model.train()\n",
        "\n",
        "    return {           \n",
        "        'pixAcc': pixAcc_score / len(loader),\n",
        "        'dice': dice_score/len(loader),\n",
        "        # 'iou': iou_score / len(loader),\n",
        "        # 'jaccard': jaccard_score / len(loader)\n",
        "    }\n",
        "\n",
        "    \n",
        "\n",
        "def test_check_acc():\n",
        "  transformer = A.Compose([\n",
        "            A.Resize(height=160, width=160),\n",
        "            #A.RandomCrop(width=500, height=500),\n",
        "            #A.Rotate(limit=15, p=.9),\n",
        "            #A.VerticalFlip(p=0.1),\n",
        "            #A.Blur(blur_limit=1, p=0.5),\n",
        "            A.Normalize(\n",
        "                mean=[0.0],\n",
        "                std=[1.0],\n",
        "                max_pixel_value=1650.0,\n",
        "            ),\n",
        "            ToTensorV2()])\n",
        "  train_loader, test_loader = get_loaders(train_images[:10], train_masks[:10], val_images, val_masks, 1, 5, transformer, transformer, 2)\n",
        "  model = ONET(in_channels=1, out_channels=1).to('cuda')\n",
        "  load_checkpoint(torch.load(f'texas/rev_onet_norm/checkpoint_13.pth.tar'), model)\n",
        "  check_accuracy(model, test_loader, 'cuda')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  pass\n",
        "  #test_check_acc()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO3cPlGWMn_9"
      },
      "source": [
        "# Saving pred images\n",
        "def summarize_performance(step, model, loader, folder=\"saved_images/\", device=\"cuda\"):\n",
        "  # Checking the accuracy\n",
        "  eval_metrics = check_accuracy(model, loader, device=device)\n",
        "  torch.cuda.empty_cache()\n",
        "  model.eval()\n",
        "  print('[*] Saving samples')\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      if torch.cuda.is_available():\n",
        "        x = x.cuda().float()\n",
        "      else:\n",
        "        x = x.float()\n",
        "      preds = torch.sigmoid(model(x))\n",
        "      n = x.shape[0]\n",
        "      preds = (preds > 0.5).float()\n",
        "      dif = (y-preds.cpu()).abs()\n",
        "      # Plot images\n",
        "      fig, axes = plt.subplots(n, 4, figsize=(12, 25))\n",
        "      \n",
        "      axes[0][0].axis(\"off\")\n",
        "      axes[0][0].invert_yaxis()\n",
        "      axes[0][0].text(0.5,0.5, \"(a) Image\", size=15, ha=\"center\", transform=axes[0][0].transAxes,  verticalalignment=\"top\", weight=\"bold\")\n",
        "      axes[0][1].axis(\"off\")\n",
        "      axes[0][1].invert_yaxis()\n",
        "      axes[0][1].text(0.5, 0.5, \"(b) Ground-Truth\", size=15, ha=\"center\", transform=axes[0][1].transAxes,  verticalalignment=\"top\", weight=\"bold\")\n",
        "      axes[0][2].axis(\"off\")\n",
        "      axes[0][2].invert_yaxis()\n",
        "      axes[0][2].text(0.5,0.5, \"(c) Predicted\", size=15, ha=\"center\", transform=axes[0][2].transAxes,  verticalalignment=\"top\", weight=\"bold\")\n",
        "      axes[0][3].axis(\"off\")\n",
        "      axes[0][3].invert_yaxis()\n",
        "      axes[0][3].text(0.5,0.5, \"(d) Difference\", size=15, ha=\"center\", transform=axes[0][3].transAxes,  verticalalignment=\"top\", weight=\"bold\")\n",
        "      print(n)\n",
        "      for i in range(1, n):\n",
        "        # Adding the real images\n",
        "        axes[i][0].imshow(x[i].cpu()[0], cmap=\"gray\", origin=\"lower\")\n",
        "        # Adding the ground-truth masks\n",
        "        axes[i][1].imshow(y[i].cpu()[0], cmap=\"gray\", origin=\"lower\")\n",
        "        # Adding the predicted masks\n",
        "        axes[i][2].imshow(preds[i].cpu()[0], cmap=\"gray\", origin=\"lower\")\n",
        "        # Adding the difference\n",
        "        axes[i][3].imshow(dif[i].cpu()[0], cmap=\"gray\", origin=\"lower\")\n",
        "      preds.detach_()\n",
        "      x.detach_()\n",
        "      break\n",
        "  # Adjust subplot parameters to give specified padding\n",
        "  plt.tight_layout()\n",
        "  # Save plots to file\n",
        "  filename = f'Summary_Step_{step}.png'\n",
        "  plt.savefig(f'{folder}/{filename}')\n",
        "  plt.close()\n",
        "  model.train()\n",
        "\n",
        "  return None\n",
        "\n",
        "def test_sum():\n",
        "  transformer = A.Compose([\n",
        "            A.Resize(height=160, width=160),\n",
        "            #A.RandomCrop(width=500, height=500),\n",
        "            #A.Rotate(limit=15, p=.9, border_mode=cv2.BORDER_CONSTANT, value=-1300),\n",
        "            #A.VerticalFlip(p=0.1),\n",
        "            #A.Blur(blur_limit=1, p=0.5),\n",
        "            ToTensorV2()])\n",
        "  _, loader = get_loaders(train_images[:10], train_masks[:10], val_images[:10], val_masks[:10], 1, 5, transformer, transformer, 2)\n",
        "  model = ONET(1,1, [8, 16]).to('cuda')\n",
        "  summarize_performance(0, model, loader, folder=\"onet/\", device='cuda')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  pass\n",
        "  # test_sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAl2bzmsYiIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3541dadd-25f1-4bea-fbdf-00a27ed681b2"
      },
      "source": [
        "covid_images = np.load('covid_images_0.npy')\n",
        "covid_masks = np.load('covid_masks_0.npy')\n",
        "covid_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 512, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaiinDToYm_d",
        "outputId": "e938accd-0de9-4d92-cb16-ddde55c47c9f"
      },
      "source": [
        "train_images = covid_images[:1100]\n",
        "val_images = covid_images[1100:]\n",
        "\n",
        "train_masks = covid_masks[:1100]\n",
        "val_masks = covid_masks[1100:]\n",
        "del covid_images, covid_masks\n",
        "val_masks.shape, train_masks.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0, 512, 512), (1000, 512, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuMB_pNGUqfH"
      },
      "source": [
        "# One training epoch\n",
        "def train_fn(epoch, model, train_loader, val_loader, optimizer, loss_fn, scaler, writer, device, check_acc = False, folder=\"testing_save/\"):\n",
        "\n",
        "  loop = tqdm(train_loader)\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate(loop):\n",
        "\n",
        "    data = data.float().to(device=device)\n",
        "    targets = targets.float().to(device=device)\n",
        "\n",
        "    # forward\n",
        "    with torch.cuda.amp.autocast():\n",
        "        predictions = model(data)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "\n",
        "    # backward\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # Logging\n",
        "    step = (epoch-1)*len(train_loader) + batch_idx\n",
        "    writer.add_scalar('Loss/train', loss, step)\n",
        "    # acc = getPixAcc(train_loader, model, device=DEVICE, iterations=1)\n",
        "    # writer.add_scalar('PixACC/train', acc, epoch)\n",
        "    # acc = getPixAcc(val_loader, model, device=DEVICE, iterations=1)\n",
        "    # writer.add_scalar('PixACC/val', acc, epoch)\n",
        "\n",
        "    # update tqdm loop\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    # Cleaning\n",
        "    loss.detach_()\n",
        "    data.detach_()\n",
        "    targets.detach_()\n",
        "    predictions.detach_()\n",
        "    del loss, data, targets, predictions\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  if check_acc:\n",
        "    summarize_performance(epoch, model, val_loader, folder=folder, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwPJhaIdU3pq"
      },
      "source": [
        "def train(DEVICE, LEARNING_RATE, NUM_EPOCHS, TRAIN_IMG, TRAIN_MASK, VAL_IMG, VAL_MASK, TRAIN_BATCH_SIZE, VAL_BATCH_SIZE, IMAGE_RES, CROP_SIZE, NUM_WORKERS, PIN_MEMORY, LOAD_MODEL='', FOLDER='testing_save/'):\n",
        "    train_transform = A.Compose([\n",
        "            A.Resize(height=IMAGE_RES, width=IMAGE_RES),\n",
        "            A.RandomCrop(width=CROP_SIZE, height=CROP_SIZE),\n",
        "            A.Rotate(limit=15, p=.9),\n",
        "            A.VerticalFlip(p=0.1),\n",
        "            #A.Blur(blur_limit=1, p=0.5),\n",
        "            ToTensorV2()])\n",
        "\n",
        "    val_transforms = A.Compose([\n",
        "            A.Resize(height=IMAGE_RES, width=IMAGE_RES),\n",
        "            #A.RandomCrop(width=CROP_SIZE, height=CROP_SIZE),\n",
        "            #A.Rotate(limit=15, p=.9),\n",
        "            #A.VerticalFlip(p=0.1),\n",
        "            #A.Blur(blur_limit=1, p=0.5),\n",
        "            ToTensorV2()])\n",
        "\n",
        "    model = ONET(in_channels=1, out_channels=1).to(DEVICE)\n",
        "    if LOAD_MODEL != '':\n",
        "      load_checkpoint(torch.load(LOAD_MODEL), model)\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    train_loader, val_loader = get_loaders(TRAIN_IMG, TRAIN_MASK, VAL_IMG, VAL_MASK, TRAIN_BATCH_SIZE, VAL_BATCH_SIZE, train_transform, val_transforms, NUM_WORKERS, PIN_MEMORY)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    WRITER = SummaryWriter(log_dir=f'tb_log/eval_master')\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('#'*50)\n",
        "      print(f'[!] Epoch: {epoch}')\n",
        "      check = (epoch+1)%2 == 0\n",
        "      train_fn(epoch+1, model, train_loader, val_loader, optimizer, loss_fn, scaler, writer=WRITER, device=DEVICE, check_acc=True, folder=FOLDER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02heGF4zVKCo"
      },
      "source": [
        "DEVICE = \"cuda\" #@param ['cpu', 'cuda']\n",
        "LEARNING_RATE = 0.0001 #@param [1e-2, 1e-3, 1e-4, 1e-5] {type: 'raw'}\n",
        "NUM_EPOCHS =   20 #@param {type: 'number'}\n",
        "TRAIN_IMG = train_images\n",
        "TRAIN_MASK = train_masks\n",
        "VAL_IMG = val_images\n",
        "VAL_MASK = val_masks\n",
        "TRAIN_BATCH_SIZE = 4 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "val_BATCH_SIZE = 8 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "IMAGE_RES =  256 #@param {type: 'number'}\n",
        "CROP_SIZE =  256 #@param {type: 'number'}\n",
        "NUM_WORKERS = 2 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "PIN_MEMORY = True #@param {type:\"boolean\"}\n",
        "LOAD_MODEL = '' #@param {type: 'string'}\n",
        "FOLDER = 'eval_master/' #@param {type: 'string'}\n",
        "\n",
        "train(\n",
        "    DEVICE,\n",
        "    LEARNING_RATE,\n",
        "    NUM_EPOCHS,\n",
        "    TRAIN_IMG,\n",
        "    TRAIN_MASK,\n",
        "    VAL_IMG,\n",
        "    VAL_MASK,\n",
        "    TRAIN_BATCH_SIZE,\n",
        "    val_BATCH_SIZE,\n",
        "    IMAGE_RES,\n",
        "    CROP_SIZE,\n",
        "    NUM_WORKERS,\n",
        "    PIN_MEMORY,\n",
        "    LOAD_MODEL,\n",
        "    FOLDER\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}